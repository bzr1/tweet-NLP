{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "624a0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3aaba124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_df = pd.read_pickle(\"./train_emb.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab45cb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb_df.columns[train_emb_df.isna().any()]  #check NAN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a12c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_df = train_emb_df.rename(columns={'TFIDF': 'emb'})\n",
    "train_emb_features = pd.DataFrame(train_emb_df['emb'].tolist())\n",
    "train_emb_labels = train_emb_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d080f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_emb_df = pd.read_pickle(\"./dev_emb.pkl\")  #dev_emb build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "838cecab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_emb_df.columns[dev_emb_df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc3ec681",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_emb_df = dev_emb_df.rename(columns={'TFIDF': 'emb'})\n",
    "dev_emb_features = pd.DataFrame(dev_emb_df['emb'].tolist())\n",
    "dev_emb_labels = dev_emb_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0dff4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test emb build\n",
    "test_emb_df = pd.read_pickle(\"./test_emb.pkl\")\n",
    "test_emb_df = test_emb_df.rename(columns={'TFIDF': 'emb'})\n",
    "test_emb_features = pd.DataFrame(test_emb_df['emb'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7a5bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tfidf build\n",
    "train_tfidf_df = pd.read_pickle(\"./train_tfidf.pkl\")\n",
    "train_tfidf_df.columns[train_tfidf_df.isna().any()]\n",
    "train_tfidf_features = pd.DataFrame(train_tfidf_df['TFIDF'].tolist())\n",
    "train_tfidf_labels = train_tfidf_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1c3ef851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev tfidf build\n",
    "dev_tfidf_df = pd.read_pickle(\"./dev_tfidf.pkl\")\n",
    "dev_tfidf_df.columns[train_tfidf_df.isna().any()]\n",
    "dev_tfidf_features = pd.DataFrame(dev_tfidf_df['TFIDF'].tolist())\n",
    "dev_tfidf_labels = dev_emb_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b93fc161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroR = DummyClassifier(strategy=\"most_frequent\")   #zeroR baseline for emb\n",
    "zeroR.fit(train_emb_features, train_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04ccca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroR.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "276329ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zeroR baseline for tfidf\n",
    "zeroR.fit(train_tfidf_features, train_tfidf_labels)\n",
    "zeroR.score(dev_tfidf_features, dev_tfidf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "74154e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 4000 points : 1541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61475"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gaussian naive bayes for emb\n",
    "gnb = GaussianNB()\n",
    "emb_gnb_pred = gnb.fit(train_emb_features, train_emb_labels).predict(dev_emb_features)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (dev_emb_features.shape[0], (dev_emb_labels != emb_gnb_pred ).sum()))\n",
    "gnb_emb=gnb.score(dev_emb_features,dev_emb_labels)\n",
    "\n",
    "gnb_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "acc53f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 4000 points : 1419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64525"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gaussian naive bayes for tfidf\n",
    "tfidf_gnb_pred = gnb.fit(train_tfidf_features, train_tfidf_labels).predict(dev_tfidf_features)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (dev_tfidf_features.shape[0], (dev_tfidf_labels != tfidf_gnb_pred).sum()))\n",
    "gnb_tfidf=gnb.score(dev_tfidf_features,dev_tfidf_labels)\n",
    "gnb_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "932d55db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69825"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression for emb\n",
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_emb_features, train_emb_labels)\n",
    "emb_lr_rpred = emb_lr_pred.predict(dev_emb_features)\n",
    "lr_emb=emb_lr_pred.score(dev_emb_features, dev_emb_labels)\n",
    "lr_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ed9ced9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67875"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression for tfidf\n",
    "tfidf_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_tfidf_features, train_tfidf_labels)\n",
    "tfidf_lr_rpred = tfidf_lr_pred.predict(dev_tfidf_features)\n",
    "lr_tfidf=tfidf_lr_pred.score(dev_tfidf_features, dev_tfidf_labels)\n",
    "lr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8e89d392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       negative\n",
       "1       positive\n",
       "2       negative\n",
       "3       negative\n",
       "4       positive\n",
       "          ...   \n",
       "3995    positive\n",
       "3996    positive\n",
       "3997    positive\n",
       "3998    positive\n",
       "3999    negative\n",
       "Name: Sentiment, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_emb_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8439f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the label to 0,1, so we can use for calculating accuracy\n",
    "zeroOneDev_emb_labels = dev_emb_labels.replace(to_replace = ['negative', 'positive'], value = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ab2c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroOneDev_tfidf_labels = dev_tfidf_labels.replace(to_replace = ['negative', 'positive'], value = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5d691a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021060259873367557"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster kmeans for emb\n",
    "kmeans_emb = KMeans(n_clusters=2, random_state=0).fit(dev_emb_features)\n",
    "#print(kmeans.labels_[:4])\n",
    "#print(zeroOneDev_emb_labels.tolist()[:4])\n",
    "completeness_score(kmeans_emb.labels_ , zeroOneDev_emb_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6251da5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020629560978054903"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster kmeans for tfidf\n",
    "kmeans_tfidf = KMeans(n_clusters=2, random_state=0).fit(dev_tfidf_features)\n",
    "completeness_score(kmeans_tfidf.labels_ , zeroOneDev_tfidf_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "2c044fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhiranbai/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/zhiranbai/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/zhiranbai/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/zhiranbai/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'activation': 'tanh', 'hidden_layer_sizes': (4, 4, 4), 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "#Multi layer neural network\n",
    "#clf = MLPClassifier(random_state=1, hidden_layer_sizes=(150,100,50), max_iter=300).fit(train_emb_features, train_emb_labels)\n",
    "\n",
    "mlp_gs = MLPClassifier(max_iter=500)\n",
    "\n",
    "#parameter_space = {\n",
    "    \n",
    "       #     'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "       #     'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "      #      'hidden_layer_sizes': [\n",
    "        #     (1,),(2,),(3,),(4,),(5,)\n",
    "        #     ]\n",
    "        \n",
    "#}\n",
    "parameter_space = {\n",
    "    \n",
    "            'activation' : [ 'tanh'],\n",
    "            'solver' : [ 'sgd'],\n",
    "            'hidden_layer_sizes': [\n",
    "             (4,4,4,),(4,4,4,1,),(4,4,4,2,),(4,4,4,4,),(4,4,4,5,),(4,4,4,7,),(4,4,4,9,)\n",
    "             ]\n",
    "        \n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(train_emb_features, train_emb_labels)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "#emb_MNN_rpred = clf.predict(dev_emb_features)\n",
    "\n",
    "\n",
    "#clf.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "598db148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69975"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multi layer neural network\n",
    "mlp = MLPClassifier(random_state=1, activation='tanh', solver='sgd', hidden_layer_sizes=(4,4,4,), max_iter=500).fit(train_emb_features, train_emb_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "emb_MNN_rpred = mlp.predict(dev_emb_features)\n",
    "\n",
    "\n",
    "mlp.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "8af84b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stacking classifier\n",
    "estimators = [\n",
    "('gnb', GaussianNB()),\n",
    "    ('rl',LogisticRegression(max_iter=1000,random_state=0)),\n",
    "    ('mlp',mlp)\n",
    "           \n",
    " ]\n",
    "stackclf = StackingClassifier(\n",
    " estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "stackmodel=stackclf.fit(train_emb_features, train_emb_labels)\n",
    "emb_stack_rpred = stackmodel.predict(dev_emb_features)\n",
    "\n",
    "\n",
    "stackmodel.score(dev_emb_features, dev_emb_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "6c3ed0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR accuracy score for emb: 0.5\n",
      "ZeroR accuracy score for tfidf: 0.5\n",
      "\n",
      "\n",
      "Gaussian naive bayes accuracy score for emb: 0.61475\n",
      "pricision: 0.6147499999999999\n",
      "recall: 0.6181032160354835\n",
      "F1:0.6175040771901504\n",
      "\n",
      "\n",
      "Gaussian naive bayes accuracy score for tfidf: 0.64525\n",
      "pricision: 0.64525\n",
      "recall: 0.6452660155782175\n",
      "F1:0.645259778066383\n",
      "\n",
      "\n",
      "Logistic regression accuracy score for emb: 0.69825\n",
      "pricision: 0.69825\n",
      "recall: 0.6985442922772279\n",
      "F1:0.6983618586850089\n",
      "\n",
      "\n",
      "Logistic regression accuracy score for tfidf: 0.67875\n",
      "pricision: 0.67875\n",
      "recall: 0.6787825831257748\n",
      "F1:0.6787646376200517\n",
      "\n",
      "\n",
      "cluster kmeans accuracy score for emb: 0.021060259873367557\n",
      "pricision: 0.578\n",
      "recall: 0.5927689019611109\n",
      "F1:0.5954918869745591\n",
      "\n",
      "\n",
      "cluster kmeans accuracy score for tfidf: 0.020629560978054903\n",
      "pricision: 0.572\n",
      "recall: 0.5981268671362219\n",
      "F1:0.6025209912623897\n",
      "\n",
      "\n",
      "multi layer neural perceptron accuracy score for emb: 0.69975\n",
      "pricision: 0.6997500000000001\n",
      "recall: 0.70158853786244\n",
      "F1:0.7004361532378921\n",
      "\n",
      "\n",
      "stacking gaussian NB and logistic Regression accuracy score for emb: 0.6995\n",
      "pricision: 0.6995\n",
      "recall: 0.6999048072346501\n",
      "F1:0.6996522051788719\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sub-summary\n",
    "print(\"ZeroR accuracy score for emb: \"+str(zeroR.score(dev_emb_features, dev_emb_labels)))\n",
    "print(\"ZeroR accuracy score for tfidf: \"+str(zeroR.score(dev_tfidf_features, dev_tfidf_labels)))\n",
    "print('\\n')\n",
    "print(\"Gaussian naive bayes accuracy score for emb: \"+str(gnb_emb))\n",
    "print('pricision: '+str(precision_score(emb_gnb_pred, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_gnb_pred, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_gnb_pred, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print(\"Gaussian naive bayes accuracy score for tfidf: \"+str(gnb_tfidf))\n",
    "print('pricision: '+str(precision_score(tfidf_gnb_pred, dev_tfidf_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(tfidf_gnb_pred, dev_tfidf_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(tfidf_gnb_pred, dev_tfidf_labels, average='weighted')))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Logistic regression accuracy score for emb: \"+str(lr_emb))\n",
    "print('pricision: '+str(precision_score(emb_lr_rpred, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_lr_rpred, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_lr_rpred, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print(\"Logistic regression accuracy score for tfidf: \"+str(lr_tfidf))\n",
    "print('pricision: '+str(precision_score(tfidf_lr_rpred, dev_tfidf_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(tfidf_lr_rpred, dev_tfidf_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(tfidf_lr_rpred, dev_tfidf_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print(\"cluster kmeans accuracy score for emb: \"+str(completeness_score(kmeans_emb.labels_ , zeroOneDev_emb_labels.tolist())))\n",
    "print('pricision: '+str(precision_score(kmeans_emb.labels_, zeroOneDev_emb_labels.tolist(), average='macro')))\n",
    "print('recall: '+str(recall_score(kmeans_emb.labels_, zeroOneDev_emb_labels.tolist(), average='macro')))\n",
    "print(\"F1:\"+str(f1_score(kmeans_emb.labels_, zeroOneDev_emb_labels.tolist(), average='weighted')))\n",
    "print('\\n')\n",
    "print(\"cluster kmeans accuracy score for tfidf: \"+str(completeness_score(kmeans_tfidf.labels_ , zeroOneDev_tfidf_labels.tolist())))\n",
    "print('pricision: '+str(precision_score(kmeans_tfidf.labels_, zeroOneDev_tfidf_labels.tolist(), average='macro')))\n",
    "print('recall: '+str(recall_score(kmeans_tfidf.labels_, zeroOneDev_tfidf_labels.tolist(), average='macro')))\n",
    "print(\"F1:\"+str(f1_score(kmeans_tfidf.labels_, zeroOneDev_tfidf_labels.tolist(), average='weighted')))\n",
    "print('\\n')\n",
    "print(\"multi layer neural perceptron accuracy score for emb: \"+str(mlp.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(emb_MNN_rpred, dev_tfidf_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_MNN_rpred, dev_tfidf_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_MNN_rpred, dev_tfidf_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print(\"stacking gaussian NB and logistic Regression accuracy score for emb: \"+str(stackmodel.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(emb_stack_rpred, dev_tfidf_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_stack_rpred, dev_tfidf_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_stack_rpred, dev_tfidf_labels, average='weighted')))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75ce88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cacaed71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 96)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature selection\n",
    "#select top 25% features\n",
    "selector25 = SelectPercentile(mutual_info_classif, percentile=25)\n",
    "train_emb_f_reduced25 = selector25.fit_transform(train_emb_features, train_emb_labels)\n",
    "train_emb_f_reduced25.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "85e97552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select top 50% features\n",
    "selector50 = SelectPercentile(mutual_info_classif, percentile=50)\n",
    "train_emb_f_reduced50 = selector50.fit_transform(train_emb_features, train_emb_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a42d0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select top 75% features\n",
    "selector75 = SelectPercentile(mutual_info_classif, percentile=75)\n",
    "train_emb_f_reduced75 = selector75.fit_transform(train_emb_features, train_emb_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a880ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68425"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression for emb after featureSelection top 25%\n",
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_emb_f_reduced25, train_emb_labels)\n",
    "dev_emb_features_reduced25 = selector25.transform(dev_emb_features)\n",
    "lr_emb_25=emb_lr_pred.score(dev_emb_features_reduced25, dev_emb_labels)\n",
    "emb_lr_rpred25 = emb_lr_pred.predict(dev_emb_features_reduced25)\n",
    "\n",
    "lr_emb_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c25ffb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression for emb after featureSelection top 50%\n",
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_emb_f_reduced50, train_emb_labels)\n",
    "dev_emb_features_reduced50 = selector50.transform(dev_emb_features)\n",
    "lr_emb_50=emb_lr_pred.score(dev_emb_features_reduced50, dev_emb_labels)\n",
    "emb_lr_rpred50 = emb_lr_pred.predict(dev_emb_features_reduced50)\n",
    "lr_emb_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "428dcc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.693"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression for emb after featureSelection top 75%\n",
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_emb_f_reduced75, train_emb_labels)\n",
    "dev_emb_features_reduced75 = selector75.transform(dev_emb_features)\n",
    "lr_emb_75=emb_lr_pred.score(dev_emb_features_reduced75, dev_emb_labels)\n",
    "emb_lr_rpred75 = emb_lr_pred.predict(dev_emb_features_reduced75)\n",
    "lr_emb_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f3b38d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression for emb after featureSelection top 25% accuracy score:0.68425\n",
      "pricision: 0.68425\n",
      "recall: 0.6846666541383998\n",
      "F1:0.6844282032521469\n",
      "\n",
      "\n",
      "Logistic regression for emb after featureSelection top 50% accuracy score:0.688\n",
      "pricision: 0.688\n",
      "recall: 0.6882718645724426\n",
      "F1:0.6881126726748354\n",
      "\n",
      "\n",
      "Logistic regression for emb after featureSelection top 75% accuracy score:0.693\n",
      "pricision: 0.6930000000000001\n",
      "recall: 0.6934837092731829\n",
      "F1:0.6931919949968731\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sub summary for feature selection\n",
    "print('Logistic regression for emb after featureSelection top 25% accuracy score:'+ str(lr_emb_25))\n",
    "print('pricision: '+str(precision_score(emb_lr_rpred25, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_lr_rpred25, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_lr_rpred25, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('Logistic regression for emb after featureSelection top 50% accuracy score:'+ str(lr_emb_50))\n",
    "print('pricision: '+str(precision_score(emb_lr_rpred50, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_lr_rpred50, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_lr_rpred50, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('Logistic regression for emb after featureSelection top 75% accuracy score:'+ str(lr_emb_75))\n",
    "print('pricision: '+str(precision_score(emb_lr_rpred75, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_lr_rpred75, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_lr_rpred75, dev_emb_labels, average='weighted')))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "460a0c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69575"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#boosting\n",
    "clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.05,\n",
    "max_depth=3, random_state=0, n_iter_no_change=20,max_features='log2').fit(train_emb_features, train_emb_labels)\n",
    "clf.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aca3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "37d0ad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.141736</td>\n",
       "      <td>-0.544553</td>\n",
       "      <td>-0.185595</td>\n",
       "      <td>-0.226111</td>\n",
       "      <td>-0.258681</td>\n",
       "      <td>-0.707513</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>-0.347276</td>\n",
       "      <td>0.306913</td>\n",
       "      <td>-0.178794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117562</td>\n",
       "      <td>-0.401929</td>\n",
       "      <td>0.283496</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>-0.071337</td>\n",
       "      <td>0.319748</td>\n",
       "      <td>0.582268</td>\n",
       "      <td>-0.480418</td>\n",
       "      <td>-0.091490</td>\n",
       "      <td>-0.270181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.159905</td>\n",
       "      <td>-0.274271</td>\n",
       "      <td>0.193665</td>\n",
       "      <td>-0.062508</td>\n",
       "      <td>0.319862</td>\n",
       "      <td>-0.233696</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>-0.243858</td>\n",
       "      <td>0.152707</td>\n",
       "      <td>0.101391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016827</td>\n",
       "      <td>-0.492055</td>\n",
       "      <td>0.451817</td>\n",
       "      <td>0.152209</td>\n",
       "      <td>-0.255176</td>\n",
       "      <td>-0.003595</td>\n",
       "      <td>0.271348</td>\n",
       "      <td>-0.077961</td>\n",
       "      <td>-0.140713</td>\n",
       "      <td>-0.099328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.184300</td>\n",
       "      <td>-0.820776</td>\n",
       "      <td>-0.198533</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.218556</td>\n",
       "      <td>-0.581000</td>\n",
       "      <td>0.685292</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235139</td>\n",
       "      <td>-0.354844</td>\n",
       "      <td>0.275892</td>\n",
       "      <td>-0.079320</td>\n",
       "      <td>-0.172949</td>\n",
       "      <td>0.141234</td>\n",
       "      <td>0.488196</td>\n",
       "      <td>-0.487415</td>\n",
       "      <td>0.268620</td>\n",
       "      <td>-0.263073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118453</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.502737</td>\n",
       "      <td>-0.263399</td>\n",
       "      <td>-0.555033</td>\n",
       "      <td>-0.100746</td>\n",
       "      <td>1.178759</td>\n",
       "      <td>0.401233</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.144832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038454</td>\n",
       "      <td>0.144181</td>\n",
       "      <td>0.059292</td>\n",
       "      <td>0.184386</td>\n",
       "      <td>-0.064873</td>\n",
       "      <td>0.448778</td>\n",
       "      <td>0.472825</td>\n",
       "      <td>-0.058554</td>\n",
       "      <td>-0.128914</td>\n",
       "      <td>-0.508223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.408979</td>\n",
       "      <td>0.232619</td>\n",
       "      <td>-0.017716</td>\n",
       "      <td>-0.248017</td>\n",
       "      <td>-0.224253</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>0.821358</td>\n",
       "      <td>0.391244</td>\n",
       "      <td>0.174423</td>\n",
       "      <td>-0.277722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441039</td>\n",
       "      <td>-0.675858</td>\n",
       "      <td>0.317422</td>\n",
       "      <td>0.092662</td>\n",
       "      <td>0.196140</td>\n",
       "      <td>0.441943</td>\n",
       "      <td>0.580520</td>\n",
       "      <td>0.128066</td>\n",
       "      <td>0.213399</td>\n",
       "      <td>-0.144494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139995</th>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.073103</td>\n",
       "      <td>-0.280805</td>\n",
       "      <td>-0.335471</td>\n",
       "      <td>-0.175296</td>\n",
       "      <td>0.156183</td>\n",
       "      <td>1.132548</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.038817</td>\n",
       "      <td>-0.126461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544730</td>\n",
       "      <td>-0.268315</td>\n",
       "      <td>-0.076898</td>\n",
       "      <td>-0.014175</td>\n",
       "      <td>-0.691903</td>\n",
       "      <td>0.097362</td>\n",
       "      <td>0.072452</td>\n",
       "      <td>0.046252</td>\n",
       "      <td>-0.308882</td>\n",
       "      <td>-0.567449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139996</th>\n",
       "      <td>-0.134720</td>\n",
       "      <td>-0.154996</td>\n",
       "      <td>-0.282151</td>\n",
       "      <td>-0.134100</td>\n",
       "      <td>-0.227636</td>\n",
       "      <td>-0.207565</td>\n",
       "      <td>0.390177</td>\n",
       "      <td>-0.184039</td>\n",
       "      <td>-0.075010</td>\n",
       "      <td>-0.255664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433176</td>\n",
       "      <td>-0.648247</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>0.253278</td>\n",
       "      <td>0.150709</td>\n",
       "      <td>-0.450959</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>0.168129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139997</th>\n",
       "      <td>-0.526653</td>\n",
       "      <td>-0.436127</td>\n",
       "      <td>-0.526713</td>\n",
       "      <td>-0.224558</td>\n",
       "      <td>0.187997</td>\n",
       "      <td>-0.368143</td>\n",
       "      <td>0.527435</td>\n",
       "      <td>0.058443</td>\n",
       "      <td>0.169143</td>\n",
       "      <td>-0.083301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277252</td>\n",
       "      <td>-0.384371</td>\n",
       "      <td>0.128579</td>\n",
       "      <td>0.161634</td>\n",
       "      <td>0.192887</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.309959</td>\n",
       "      <td>-0.494707</td>\n",
       "      <td>0.689926</td>\n",
       "      <td>-0.153628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139998</th>\n",
       "      <td>-0.293092</td>\n",
       "      <td>-0.206648</td>\n",
       "      <td>-0.210050</td>\n",
       "      <td>-0.214401</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>0.184987</td>\n",
       "      <td>0.299076</td>\n",
       "      <td>0.236207</td>\n",
       "      <td>-0.453665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493244</td>\n",
       "      <td>-0.332592</td>\n",
       "      <td>0.038494</td>\n",
       "      <td>0.451940</td>\n",
       "      <td>0.044022</td>\n",
       "      <td>0.272157</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>-0.095991</td>\n",
       "      <td>-0.050408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139999</th>\n",
       "      <td>0.001131</td>\n",
       "      <td>-0.266324</td>\n",
       "      <td>-0.055139</td>\n",
       "      <td>-0.146689</td>\n",
       "      <td>-0.171287</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.776733</td>\n",
       "      <td>-0.354036</td>\n",
       "      <td>-0.093335</td>\n",
       "      <td>-0.058569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215490</td>\n",
       "      <td>-0.147267</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.158016</td>\n",
       "      <td>-0.261621</td>\n",
       "      <td>0.323057</td>\n",
       "      <td>0.309146</td>\n",
       "      <td>-0.616134</td>\n",
       "      <td>0.152467</td>\n",
       "      <td>-0.290610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.141736 -0.544553 -0.185595 -0.226111 -0.258681 -0.707513  0.827305   \n",
       "1      -0.159905 -0.274271  0.193665 -0.062508  0.319862 -0.233696  0.547739   \n",
       "2      -0.184300 -0.820776 -0.198533 -0.126704 -0.218556 -0.581000  0.685292   \n",
       "3      -0.118453  0.008326  0.502737 -0.263399 -0.555033 -0.100746  1.178759   \n",
       "4      -0.408979  0.232619 -0.017716 -0.248017 -0.224253 -0.033788  0.821358   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "139995  0.007890  0.073103 -0.280805 -0.335471 -0.175296  0.156183  1.132548   \n",
       "139996 -0.134720 -0.154996 -0.282151 -0.134100 -0.227636 -0.207565  0.390177   \n",
       "139997 -0.526653 -0.436127 -0.526713 -0.224558  0.187997 -0.368143  0.527435   \n",
       "139998 -0.293092 -0.206648 -0.210050 -0.214401  0.000266  0.173398  0.184987   \n",
       "139999  0.001131 -0.266324 -0.055139 -0.146689 -0.171287  0.004098  0.776733   \n",
       "\n",
       "             7         8         9    ...       374       375       376  \\\n",
       "0      -0.347276  0.306913 -0.178794  ...  0.117562 -0.401929  0.283496   \n",
       "1      -0.243858  0.152707  0.101391  ... -0.016827 -0.492055  0.451817   \n",
       "2       0.000823  0.169189  0.041687  ... -0.235139 -0.354844  0.275892   \n",
       "3       0.401233  0.298557  0.144832  ... -0.038454  0.144181  0.059292   \n",
       "4       0.391244  0.174423 -0.277722  ...  0.441039 -0.675858  0.317422   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "139995  0.301846  0.038817 -0.126461  ...  0.544730 -0.268315 -0.076898   \n",
       "139996 -0.184039 -0.075010 -0.255664  ...  0.433176 -0.648247  0.116928   \n",
       "139997  0.058443  0.169143 -0.083301  ... -0.277252 -0.384371  0.128579   \n",
       "139998  0.299076  0.236207 -0.453665  ...  0.493244 -0.332592  0.038494   \n",
       "139999 -0.354036 -0.093335 -0.058569  ... -0.215490 -0.147267  0.185972   \n",
       "\n",
       "             377       378       379       380       381       382       383  \n",
       "0      -0.023253 -0.071337  0.319748  0.582268 -0.480418 -0.091490 -0.270181  \n",
       "1       0.152209 -0.255176 -0.003595  0.271348 -0.077961 -0.140713 -0.099328  \n",
       "2      -0.079320 -0.172949  0.141234  0.488196 -0.487415  0.268620 -0.263073  \n",
       "3       0.184386 -0.064873  0.448778  0.472825 -0.058554 -0.128914 -0.508223  \n",
       "4       0.092662  0.196140  0.441943  0.580520  0.128066  0.213399 -0.144494  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "139995 -0.014175 -0.691903  0.097362  0.072452  0.046252 -0.308882 -0.567449  \n",
       "139996  0.025290 -0.006734  0.253278  0.150709 -0.450959 -0.343677  0.168129  \n",
       "139997  0.161634  0.192887  0.016388  0.309959 -0.494707  0.689926 -0.153628  \n",
       "139998  0.451940  0.044022  0.272157 -0.041516  0.038367 -0.095991 -0.050408  \n",
       "139999  0.158016 -0.261621  0.323057  0.309146 -0.616134  0.152467 -0.290610  \n",
       "\n",
       "[140000 rows x 384 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi-supervise --combine features\n",
    "unlabeled_emb_df = pd.read_pickle(\"./unlabeled_emb.pkl\")  \n",
    "ublabeled_emb_features = pd.DataFrame(unlabeled_emb_df['TFIDF'].tolist())\n",
    "ublabeled_emb_labels = pd.DataFrame(unlabeled_emb_df['Sentiment'].tolist())\n",
    "\n",
    "combinef = pd.concat([train_emb_features,ublabeled_emb_features])\n",
    "combinef.reset_index(drop=True,inplace=True)\n",
    "combinef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f2e1856d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine label and unlabel\n",
    "ublabeled_emb_labels=ublabeled_emb_labels.fillna(-1)  #semi sklearn need unlabled to be -1\n",
    "ublabeled_emb_labels\n",
    "combinel =pd.concat([train_emb_labels,ublabeled_emb_labels]) \n",
    "combinel.reset_index(drop=True,inplace=True)\n",
    "combinel.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7ee51bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi model with logestic regression build & predict w threshold 0.75\n",
    "lr = LogisticRegression(max_iter=2000,random_state=0)\n",
    "semi_lr_model = SelfTrainingClassifier(lr)\n",
    "semi_lr_model.fit(combinef, combinel.values.ravel())\n",
    "emb_semilr_rpred75 = semi_lr_model.predict(dev_emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3973fa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70275"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_lr_model.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "20d78333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    57829\n",
       "negative    52543\n",
       "-1          29628\n",
       "dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(semi_lr_model.transduction_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6daab83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=LogisticRegression(max_iter=1000,\n",
       "                                                         random_state=0),\n",
       "                       threshold=0.9)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi model with logestic regression build & predict w threshold 0.9\n",
    "self_training_model9 = SelfTrainingClassifier(lr,threshold=0.9)\n",
    "self_training_model9.fit(combinef, combinel.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "cb0d3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_semilr_rpred90= self_training_model9.predict(dev_emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e75e12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_training_model9.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aae7cfeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=LogisticRegression(max_iter=1000,\n",
       "                                                         random_state=0),\n",
       "                       threshold=0.65)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi model with logestic regression build & predict w threshold 0.65\n",
    "self_training_model65 = SelfTrainingClassifier(lr,threshold=0.65)\n",
    "self_training_model65.fit(combinef, combinel.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6329fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_semilr_rpred65= self_training_model65.predict(dev_emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "486ba078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7015"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_training_model65.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9c69faa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=GaussianNB())"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi model with naive bayes build & predict w threshold 0.75\n",
    "gnb = GaussianNB()\n",
    "semi_gnb_model = SelfTrainingClassifier(gnb)\n",
    "semi_gnb_model.fit(combinef, combinel.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "410f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_gnb_model75pre=semi_gnb_model.predict(dev_emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "06eecc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58825"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_gnb_model.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fbc86d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    78748\n",
       "positive    61224\n",
       "-1             28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(semi_gnb_model.transduction_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "35919d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=GaussianNB(), threshold=0.85)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi model with naive bayes build & predict w threshold 0.85\n",
    "gnb = GaussianNB()\n",
    "semi_gnb_model85 = SelfTrainingClassifier(gnb,threshold=0.85)\n",
    "semi_gnb_model85.fit(combinef, combinel.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "47eb0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_gnb_model85pre=semi_gnb_model85.predict(dev_emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "38057f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.588"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_gnb_model85.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c5294efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    79522\n",
       "positive    60424\n",
       "-1             54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(semi_gnb_model85.transduction_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "a9b926ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63725"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#semi model with multi layer percepcion build & predict w threshold 0.75\n",
    "model = mlp_gs\n",
    "semi_MLP_model = SelfTrainingClassifier(model)\n",
    "semi_MLP_model.fit(combinef, combinel.values.ravel())\n",
    "semi_MLP_model75pre=semi_MLP_model.predict(dev_emb_features)\n",
    "semi_MLP_model.score(dev_emb_features, dev_emb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "02f3d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#semi model with logestic regression w threshold 0.75: 0.70275\n",
      "pricision: 0.70275\n",
      "recall: 0.7033993014199578\n",
      "F1:0.7029874135489754\n",
      "\n",
      "\n",
      "#semi model with logestic regression w threshold 0.9: 0.697\n",
      "pricision: 0.6970000000000001\n",
      "recall: 0.6971232020012508\n",
      "F1:0.6970473511486169\n",
      "\n",
      "\n",
      "#semi model with logestic regression w threshold 0.65: 0.7015\n",
      "pricision: 0.7015\n",
      "recall: 0.7023287385129491\n",
      "F1:0.7018059773207765\n",
      "\n",
      "\n",
      "#semi model with gaussianNB w threshold 0.75: 0.58825\n",
      "pricision: 0.5882499999999999\n",
      "recall: 0.5982827251544472\n",
      "F1:0.5990330703640063\n",
      "\n",
      "\n",
      "#semi model with gaussianNB w threshold 0.85: 0.588\n",
      "pricision: 0.5880000000000001\n",
      "recall: 0.5981092781275399\n",
      "F1:0.5988938513338221\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#semi-summary\n",
    "print('#semi model with logestic regression w threshold 0.75: '+str(semi_lr_model.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(emb_semilr_rpred75, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_semilr_rpred75, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_semilr_rpred75, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('#semi model with logestic regression w threshold 0.9: '+str(self_training_model9.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(emb_semilr_rpred90, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_semilr_rpred90, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_semilr_rpred90, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('#semi model with logestic regression w threshold 0.65: '+str(self_training_model65.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(emb_semilr_rpred65, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(emb_semilr_rpred65, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(emb_semilr_rpred65, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('#semi model with gaussianNB w threshold 0.75: '+str(semi_gnb_model.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(semi_gnb_model75pre, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(semi_gnb_model75pre, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(semi_gnb_model75pre, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('#semi model with gaussianNB w threshold 0.85: '+str(semi_gnb_model85.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(semi_gnb_model85pre, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(semi_gnb_model85pre, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(semi_gnb_model85pre, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "print('#semi model with multi layer percepcion w threshold 0.85: '+str(semi_gnb_model85.score(dev_emb_features, dev_emb_labels)))\n",
    "print('pricision: '+str(precision_score(semi_gnb_model85pre, dev_emb_labels, average='macro')))\n",
    "print('recall: '+str(recall_score(semi_gnb_model85pre, dev_emb_labels, average='macro')))\n",
    "print(\"F1:\"+str(f1_score(semi_gnb_model85pre, dev_emb_labels, average='weighted')))\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f68dd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature Engineering\n",
    "train_tweet_df = pd.read_pickle(\"./train.pkl\")\n",
    "train_tweetdev_df = pd.read_pickle(\"./dev.pkl\")\n",
    "train_tweet_df['text']\n",
    "train_tweet = train_tweet_df['text'].tolist()\n",
    "train_tweetdev = train_tweetdev_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d89eafed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "polarity =[]\n",
    "devpolarity = []\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "for i in train_tweet:\n",
    "    polarity.append(analyser.polarity_scores(i))\n",
    "for i in train_tweetdev:\n",
    "    devpolarity.append(analyser.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8883034f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0.213</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         neg    neu    pos  compound\n",
       "0      0.000  0.641  0.359    0.4215\n",
       "1      0.000  1.000  0.000    0.0000\n",
       "2      0.000  1.000  0.000    0.0000\n",
       "3      0.000  0.682  0.318    0.6369\n",
       "4      0.137  0.863  0.000   -0.2263\n",
       "...      ...    ...    ...       ...\n",
       "39995  0.000  1.000  0.000    0.0000\n",
       "39996  0.333  0.667  0.000   -0.6115\n",
       "39997  0.213  0.787  0.000   -0.5848\n",
       "39998  0.000  0.678  0.322    0.2263\n",
       "39999  0.000  1.000  0.000    0.0000\n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarityf = pd.DataFrame(polarity)\n",
    "devpolarityf=pd.DataFrame(devpolarity)\n",
    "polarityf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "6911414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.141736</td>\n",
       "      <td>-0.544553</td>\n",
       "      <td>-0.185595</td>\n",
       "      <td>-0.226111</td>\n",
       "      <td>-0.258681</td>\n",
       "      <td>-0.707513</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>-0.347276</td>\n",
       "      <td>0.306913</td>\n",
       "      <td>-0.178794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117562</td>\n",
       "      <td>-0.401929</td>\n",
       "      <td>0.283496</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>-0.071337</td>\n",
       "      <td>0.319748</td>\n",
       "      <td>0.582268</td>\n",
       "      <td>-0.480418</td>\n",
       "      <td>-0.091490</td>\n",
       "      <td>-0.270181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.159905</td>\n",
       "      <td>-0.274271</td>\n",
       "      <td>0.193665</td>\n",
       "      <td>-0.062508</td>\n",
       "      <td>0.319862</td>\n",
       "      <td>-0.233696</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>-0.243858</td>\n",
       "      <td>0.152707</td>\n",
       "      <td>0.101391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016827</td>\n",
       "      <td>-0.492055</td>\n",
       "      <td>0.451817</td>\n",
       "      <td>0.152209</td>\n",
       "      <td>-0.255176</td>\n",
       "      <td>-0.003595</td>\n",
       "      <td>0.271348</td>\n",
       "      <td>-0.077961</td>\n",
       "      <td>-0.140713</td>\n",
       "      <td>-0.099328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.184300</td>\n",
       "      <td>-0.820776</td>\n",
       "      <td>-0.198533</td>\n",
       "      <td>-0.126704</td>\n",
       "      <td>-0.218556</td>\n",
       "      <td>-0.581000</td>\n",
       "      <td>0.685292</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235139</td>\n",
       "      <td>-0.354844</td>\n",
       "      <td>0.275892</td>\n",
       "      <td>-0.079320</td>\n",
       "      <td>-0.172949</td>\n",
       "      <td>0.141234</td>\n",
       "      <td>0.488196</td>\n",
       "      <td>-0.487415</td>\n",
       "      <td>0.268620</td>\n",
       "      <td>-0.263073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118453</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.502737</td>\n",
       "      <td>-0.263399</td>\n",
       "      <td>-0.555033</td>\n",
       "      <td>-0.100746</td>\n",
       "      <td>1.178759</td>\n",
       "      <td>0.401233</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.144832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038454</td>\n",
       "      <td>0.144181</td>\n",
       "      <td>0.059292</td>\n",
       "      <td>0.184386</td>\n",
       "      <td>-0.064873</td>\n",
       "      <td>0.448778</td>\n",
       "      <td>0.472825</td>\n",
       "      <td>-0.058554</td>\n",
       "      <td>-0.128914</td>\n",
       "      <td>-0.508223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.408979</td>\n",
       "      <td>0.232619</td>\n",
       "      <td>-0.017716</td>\n",
       "      <td>-0.248017</td>\n",
       "      <td>-0.224253</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>0.821358</td>\n",
       "      <td>0.391244</td>\n",
       "      <td>0.174423</td>\n",
       "      <td>-0.277722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441039</td>\n",
       "      <td>-0.675858</td>\n",
       "      <td>0.317422</td>\n",
       "      <td>0.092662</td>\n",
       "      <td>0.196140</td>\n",
       "      <td>0.441943</td>\n",
       "      <td>0.580520</td>\n",
       "      <td>0.128066</td>\n",
       "      <td>0.213399</td>\n",
       "      <td>-0.144494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>-0.430240</td>\n",
       "      <td>-0.382024</td>\n",
       "      <td>-0.316330</td>\n",
       "      <td>-0.309180</td>\n",
       "      <td>0.120179</td>\n",
       "      <td>-0.428284</td>\n",
       "      <td>0.831630</td>\n",
       "      <td>-0.048464</td>\n",
       "      <td>0.182245</td>\n",
       "      <td>0.094724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013516</td>\n",
       "      <td>-0.120240</td>\n",
       "      <td>0.318467</td>\n",
       "      <td>0.132105</td>\n",
       "      <td>-0.052484</td>\n",
       "      <td>-0.055811</td>\n",
       "      <td>0.305198</td>\n",
       "      <td>-0.442307</td>\n",
       "      <td>0.294606</td>\n",
       "      <td>-0.031146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>-0.338988</td>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.209065</td>\n",
       "      <td>-0.207502</td>\n",
       "      <td>-0.181979</td>\n",
       "      <td>-0.258415</td>\n",
       "      <td>0.691391</td>\n",
       "      <td>0.322848</td>\n",
       "      <td>0.253605</td>\n",
       "      <td>-0.090219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095638</td>\n",
       "      <td>-0.143568</td>\n",
       "      <td>0.266862</td>\n",
       "      <td>0.326890</td>\n",
       "      <td>-0.208161</td>\n",
       "      <td>0.205226</td>\n",
       "      <td>0.805715</td>\n",
       "      <td>-0.275289</td>\n",
       "      <td>0.198044</td>\n",
       "      <td>0.012185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>-0.293665</td>\n",
       "      <td>0.241879</td>\n",
       "      <td>-0.232575</td>\n",
       "      <td>-0.098800</td>\n",
       "      <td>-0.169655</td>\n",
       "      <td>-0.621847</td>\n",
       "      <td>0.139723</td>\n",
       "      <td>0.340485</td>\n",
       "      <td>0.038315</td>\n",
       "      <td>-0.112721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122672</td>\n",
       "      <td>-0.897764</td>\n",
       "      <td>0.242542</td>\n",
       "      <td>0.295738</td>\n",
       "      <td>-0.303057</td>\n",
       "      <td>0.061978</td>\n",
       "      <td>0.163468</td>\n",
       "      <td>-0.160094</td>\n",
       "      <td>-0.040386</td>\n",
       "      <td>-0.061228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>-0.034713</td>\n",
       "      <td>-0.587140</td>\n",
       "      <td>-0.586187</td>\n",
       "      <td>0.073634</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>-0.229916</td>\n",
       "      <td>-0.416593</td>\n",
       "      <td>0.191784</td>\n",
       "      <td>0.398879</td>\n",
       "      <td>-0.663269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450470</td>\n",
       "      <td>-0.237810</td>\n",
       "      <td>0.212717</td>\n",
       "      <td>-0.538136</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>0.078669</td>\n",
       "      <td>-0.449346</td>\n",
       "      <td>-0.086853</td>\n",
       "      <td>-0.273231</td>\n",
       "      <td>-0.576859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.215718</td>\n",
       "      <td>-0.221756</td>\n",
       "      <td>-0.333251</td>\n",
       "      <td>0.276927</td>\n",
       "      <td>0.056056</td>\n",
       "      <td>-0.007197</td>\n",
       "      <td>0.325420</td>\n",
       "      <td>0.552308</td>\n",
       "      <td>0.047321</td>\n",
       "      <td>0.089098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119569</td>\n",
       "      <td>0.251944</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.392031</td>\n",
       "      <td>-0.454584</td>\n",
       "      <td>0.327771</td>\n",
       "      <td>-0.152575</td>\n",
       "      <td>-0.023988</td>\n",
       "      <td>0.134552</td>\n",
       "      <td>0.041586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.141736 -0.544553 -0.185595 -0.226111 -0.258681 -0.707513  0.827305   \n",
       "1     -0.159905 -0.274271  0.193665 -0.062508  0.319862 -0.233696  0.547739   \n",
       "2     -0.184300 -0.820776 -0.198533 -0.126704 -0.218556 -0.581000  0.685292   \n",
       "3     -0.118453  0.008326  0.502737 -0.263399 -0.555033 -0.100746  1.178759   \n",
       "4     -0.408979  0.232619 -0.017716 -0.248017 -0.224253 -0.033788  0.821358   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995 -0.430240 -0.382024 -0.316330 -0.309180  0.120179 -0.428284  0.831630   \n",
       "39996 -0.338988 -0.482711  0.209065 -0.207502 -0.181979 -0.258415  0.691391   \n",
       "39997 -0.293665  0.241879 -0.232575 -0.098800 -0.169655 -0.621847  0.139723   \n",
       "39998 -0.034713 -0.587140 -0.586187  0.073634  0.016169 -0.229916 -0.416593   \n",
       "39999  0.215718 -0.221756 -0.333251  0.276927  0.056056 -0.007197  0.325420   \n",
       "\n",
       "            7         8         9    ...       374       375       376  \\\n",
       "0     -0.347276  0.306913 -0.178794  ...  0.117562 -0.401929  0.283496   \n",
       "1     -0.243858  0.152707  0.101391  ... -0.016827 -0.492055  0.451817   \n",
       "2      0.000823  0.169189  0.041687  ... -0.235139 -0.354844  0.275892   \n",
       "3      0.401233  0.298557  0.144832  ... -0.038454  0.144181  0.059292   \n",
       "4      0.391244  0.174423 -0.277722  ...  0.441039 -0.675858  0.317422   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "39995 -0.048464  0.182245  0.094724  ... -0.013516 -0.120240  0.318467   \n",
       "39996  0.322848  0.253605 -0.090219  ... -0.095638 -0.143568  0.266862   \n",
       "39997  0.340485  0.038315 -0.112721  ...  0.122672 -0.897764  0.242542   \n",
       "39998  0.191784  0.398879 -0.663269  ...  0.450470 -0.237810  0.212717   \n",
       "39999  0.552308  0.047321  0.089098  ...  0.119569  0.251944  0.103894   \n",
       "\n",
       "            377       378       379       380       381       382       383  \n",
       "0     -0.023253 -0.071337  0.319748  0.582268 -0.480418 -0.091490 -0.270181  \n",
       "1      0.152209 -0.255176 -0.003595  0.271348 -0.077961 -0.140713 -0.099328  \n",
       "2     -0.079320 -0.172949  0.141234  0.488196 -0.487415  0.268620 -0.263073  \n",
       "3      0.184386 -0.064873  0.448778  0.472825 -0.058554 -0.128914 -0.508223  \n",
       "4      0.092662  0.196140  0.441943  0.580520  0.128066  0.213399 -0.144494  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "39995  0.132105 -0.052484 -0.055811  0.305198 -0.442307  0.294606 -0.031146  \n",
       "39996  0.326890 -0.208161  0.205226  0.805715 -0.275289  0.198044  0.012185  \n",
       "39997  0.295738 -0.303057  0.061978  0.163468 -0.160094 -0.040386 -0.061228  \n",
       "39998 -0.538136  0.020622  0.078669 -0.449346 -0.086853 -0.273231 -0.576859  \n",
       "39999 -0.392031 -0.454584  0.327771 -0.152575 -0.023988  0.134552  0.041586  \n",
       "\n",
       "[40000 rows x 384 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "24ca6225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.647439</td>\n",
       "      <td>-0.160090</td>\n",
       "      <td>-0.038481</td>\n",
       "      <td>-0.055743</td>\n",
       "      <td>-0.026119</td>\n",
       "      <td>-0.319144</td>\n",
       "      <td>0.016490</td>\n",
       "      <td>-0.523501</td>\n",
       "      <td>-0.423618</td>\n",
       "      <td>-0.144300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031740</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>0.070147</td>\n",
       "      <td>-0.056241</td>\n",
       "      <td>-0.597363</td>\n",
       "      <td>-0.147419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.464021</td>\n",
       "      <td>-0.016592</td>\n",
       "      <td>-0.115892</td>\n",
       "      <td>0.066312</td>\n",
       "      <td>-0.375523</td>\n",
       "      <td>-0.073790</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.215952</td>\n",
       "      <td>-0.256112</td>\n",
       "      <td>0.171260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376908</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>-0.743679</td>\n",
       "      <td>0.213966</td>\n",
       "      <td>0.108167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213534</td>\n",
       "      <td>-0.138032</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.138779</td>\n",
       "      <td>0.597589</td>\n",
       "      <td>-0.454876</td>\n",
       "      <td>-0.455043</td>\n",
       "      <td>0.716374</td>\n",
       "      <td>0.346029</td>\n",
       "      <td>0.130042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134962</td>\n",
       "      <td>0.589568</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>0.311702</td>\n",
       "      <td>-0.610152</td>\n",
       "      <td>0.472914</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.116541</td>\n",
       "      <td>0.066684</td>\n",
       "      <td>-0.162548</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>-0.324099</td>\n",
       "      <td>-0.180485</td>\n",
       "      <td>1.083803</td>\n",
       "      <td>-0.622489</td>\n",
       "      <td>0.022407</td>\n",
       "      <td>-0.292906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295738</td>\n",
       "      <td>0.179404</td>\n",
       "      <td>0.937333</td>\n",
       "      <td>-0.181149</td>\n",
       "      <td>-0.435988</td>\n",
       "      <td>-0.094660</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073845</td>\n",
       "      <td>0.119823</td>\n",
       "      <td>-0.140898</td>\n",
       "      <td>-0.030531</td>\n",
       "      <td>-0.156140</td>\n",
       "      <td>-0.122156</td>\n",
       "      <td>0.522053</td>\n",
       "      <td>0.091225</td>\n",
       "      <td>0.184444</td>\n",
       "      <td>-0.130128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309555</td>\n",
       "      <td>-0.061245</td>\n",
       "      <td>0.555611</td>\n",
       "      <td>-0.190059</td>\n",
       "      <td>-0.118419</td>\n",
       "      <td>0.462156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.330185</td>\n",
       "      <td>0.128467</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>-0.603940</td>\n",
       "      <td>-0.680551</td>\n",
       "      <td>0.436781</td>\n",
       "      <td>0.587363</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.072278</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.076681</td>\n",
       "      <td>0.068563</td>\n",
       "      <td>0.018803</td>\n",
       "      <td>-0.404385</td>\n",
       "      <td>-0.406231</td>\n",
       "      <td>0.345451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.268135</td>\n",
       "      <td>-0.193721</td>\n",
       "      <td>0.505916</td>\n",
       "      <td>0.017008</td>\n",
       "      <td>0.780905</td>\n",
       "      <td>-0.351398</td>\n",
       "      <td>0.126403</td>\n",
       "      <td>0.176521</td>\n",
       "      <td>-0.070430</td>\n",
       "      <td>-0.027673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309937</td>\n",
       "      <td>-0.249904</td>\n",
       "      <td>0.387103</td>\n",
       "      <td>-0.089303</td>\n",
       "      <td>-0.185726</td>\n",
       "      <td>0.358549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>-0.410717</td>\n",
       "      <td>-0.416210</td>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.155333</td>\n",
       "      <td>0.037716</td>\n",
       "      <td>-0.662711</td>\n",
       "      <td>0.769891</td>\n",
       "      <td>0.196149</td>\n",
       "      <td>0.079130</td>\n",
       "      <td>-0.289609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249280</td>\n",
       "      <td>0.235375</td>\n",
       "      <td>0.659818</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>0.191358</td>\n",
       "      <td>-0.102210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-0.138316</td>\n",
       "      <td>0.229265</td>\n",
       "      <td>0.170203</td>\n",
       "      <td>-0.347052</td>\n",
       "      <td>-0.093248</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.594051</td>\n",
       "      <td>-0.099069</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>-0.444591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083219</td>\n",
       "      <td>0.075853</td>\n",
       "      <td>0.293239</td>\n",
       "      <td>0.207108</td>\n",
       "      <td>0.382951</td>\n",
       "      <td>-0.078980</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.2942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.058211</td>\n",
       "      <td>-0.363893</td>\n",
       "      <td>0.171643</td>\n",
       "      <td>-0.113713</td>\n",
       "      <td>-0.287586</td>\n",
       "      <td>0.040460</td>\n",
       "      <td>0.581539</td>\n",
       "      <td>-0.420158</td>\n",
       "      <td>0.206981</td>\n",
       "      <td>0.101092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113025</td>\n",
       "      <td>0.429121</td>\n",
       "      <td>0.348487</td>\n",
       "      <td>-0.019468</td>\n",
       "      <td>-0.467313</td>\n",
       "      <td>-0.095094</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.8338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.647439 -0.160090 -0.038481 -0.055743 -0.026119 -0.319144  0.016490   \n",
       "1    -0.464021 -0.016592 -0.115892  0.066312 -0.375523 -0.073790  0.489918   \n",
       "2    -0.213534 -0.138032  0.006368  0.138779  0.597589 -0.454876 -0.455043   \n",
       "3    -0.116541  0.066684 -0.162548  0.283333 -0.324099 -0.180485  1.083803   \n",
       "4    -0.073845  0.119823 -0.140898 -0.030531 -0.156140 -0.122156  0.522053   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3995  0.330185  0.128467  0.135211 -0.603940 -0.680551  0.436781  0.587363   \n",
       "3996  0.268135 -0.193721  0.505916  0.017008  0.780905 -0.351398  0.126403   \n",
       "3997 -0.410717 -0.416210 -0.008091 -0.155333  0.037716 -0.662711  0.769891   \n",
       "3998 -0.138316  0.229265  0.170203 -0.347052 -0.093248  0.015622  0.594051   \n",
       "3999 -0.058211 -0.363893  0.171643 -0.113713 -0.287586  0.040460  0.581539   \n",
       "\n",
       "             7         8         9  ...       378       379       380  \\\n",
       "0    -0.523501 -0.423618 -0.144300  ...  0.031740  0.009924  0.070147   \n",
       "1     0.215952 -0.256112  0.171260  ...  0.376908  0.371955  0.634615   \n",
       "2     0.716374  0.346029  0.130042  ... -0.134962  0.589568 -0.023502   \n",
       "3    -0.622489  0.022407 -0.292906  ... -0.295738  0.179404  0.937333   \n",
       "4     0.091225  0.184444 -0.130128  ... -0.309555 -0.061245  0.555611   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3995  0.700500  0.072278 -0.053482  ... -1.076681  0.068563  0.018803   \n",
       "3996  0.176521 -0.070430 -0.027673  ... -0.309937 -0.249904  0.387103   \n",
       "3997  0.196149  0.079130 -0.289609  ... -0.249280  0.235375  0.659818   \n",
       "3998 -0.099069  0.197505 -0.444591  ...  0.083219  0.075853  0.293239   \n",
       "3999 -0.420158  0.206981  0.101092  ... -0.113025  0.429121  0.348487   \n",
       "\n",
       "           381       382       383    neg    neu    pos  compound  \n",
       "0    -0.056241 -0.597363 -0.147419  0.000  1.000  0.000    0.0000  \n",
       "1    -0.743679  0.213966  0.108167  0.000  1.000  0.000    0.0000  \n",
       "2     0.311702 -0.610152  0.472914  0.000  1.000  0.000    0.0000  \n",
       "3    -0.181149 -0.435988 -0.094660  0.473  0.310  0.217   -0.5106  \n",
       "4    -0.190059 -0.118419  0.462156  0.000  1.000  0.000    0.0000  \n",
       "...        ...       ...       ...    ...    ...    ...       ...  \n",
       "3995 -0.404385 -0.406231  0.345451  0.000  1.000  0.000    0.0000  \n",
       "3996 -0.089303 -0.185726  0.358549  0.000  1.000  0.000    0.0000  \n",
       "3997  0.029833  0.191358 -0.102210  0.000  0.818  0.182    0.4404  \n",
       "3998  0.207108  0.382951 -0.078980  0.000  0.872  0.128    0.2942  \n",
       "3999 -0.019468 -0.467313 -0.095094  0.246  0.754  0.000   -0.8338  \n",
       "\n",
       "[4000 rows x 388 columns]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb_features_more = pd.concat([train_emb_features, polarityf], axis=1)\n",
    "dev_emb_features_more = pd.concat([dev_emb_features, devpolarityf], axis=1)\n",
    "dev_emb_features_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2944b59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69175"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(train_emb_features_more, train_emb_labels)\n",
    "emb_lr_rpred = emb_lr_pred.predict(dev_emb_features_more)\n",
    "lr_emb=emb_lr_pred.score(dev_emb_features_more, dev_emb_labels)\n",
    "lr_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0c3ee20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 4000 points : 1519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62025"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_gnb_pred = gnb.fit(train_emb_features_more, train_emb_labels).predict(dev_emb_features_more)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (dev_emb_features_more.shape[0], (dev_emb_labels != emb_gnb_pred ).sum()))\n",
    "gnb_emb=gnb.score(dev_emb_features_more,dev_emb_labels)\n",
    "gnb_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "40130e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5695"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lr_pred = LogisticRegression(max_iter=1000,random_state=0).fit(polarityf, train_emb_labels)\n",
    "emb_lr_rpred = emb_lr_pred.predict(devpolarityf)\n",
    "lr_emb=emb_lr_pred.score(devpolarityf, dev_emb_labels)\n",
    "lr_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5afd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2aa16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a91f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c3c08f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=semi_lr_model.predict(test_emb_features)\n",
    "kaggle = pd.DataFrame(kaggle, columns = ['Category'])\n",
    "\n",
    "kaggle.to_csv(\"kaggle.csv\",header=True ,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60392df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
